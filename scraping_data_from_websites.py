# -*- coding: utf-8 -*-
"""Scraping Data from Websites.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1DM93NCrW8kOZ52n8OxLwDTEDNcaaxC

## **Loading Libraries**
"""

from urllib.request import urlopen
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests
import pandas as pd
import sys
import time

def returnn(a):
  try:
    return a.string
  except:
    return str(a)

"""## **Amazon Scraping : Scraping New info from Amazon**"""

url_p1 = 'https://www.amazon.in/s?k='
url_p2='&page='
kwords=['facemasks']#,'ppe', 'handsanitizers','handwash']
pages=[1]#,2,3,4,5,6,7,8,9]
amz = pd.DataFrame(columns=['url','product', 'id', 'name', 'mrp', 'discounted'])
for k in kwords:
  for j in pages:
    try:
      quote_page=str(url_p1)+str(k)+str(url_p2)+str(j)
      time.sleep(2)
      page = urlopen(quote_page)
      soup = BeautifulSoup(page, 'html.parser')
      raw_urls = soup.find_all('a', attrs={'class': 'a-link-normal a-text-normal'})
      raw_urls=list(raw_urls)
      for i in raw_urls:
          try:
              time.sleep(2)
              r = requests.get('https://www.amazon.in/' + str(i['href']))
              print(r.url)
              quote_page = str(r.url)
              page = urlopen(quote_page)
              soup = BeautifulSoup(page, 'html.parser')
              pd_name = soup.find('span', attrs={'class': 'a-size-large product-title-word-break'})
              try:
                pd_mrp = soup.find('span', attrs={'class': 'priceBlockStrikePriceString a-text-strike'})
              except:
                pd_mrp='nill'
              try:
                pd_price = soup.find('span', attrs={'class': 'a-size-medium a-color-price priceBlockBuyingPriceString'})
              except:
                pd_price='nill'
              pd_code=r.url
              pd_code=pd_code.split('/')
              pd_code=pd_code[-1].split('?')[0]
              listt=[r.url, k, pd_code, pd_name.string.strip(), returnn(pd_mrp), returnn(pd_price)]
              listt = pd.Series(listt, index = amz.columns)
              amz = amz.append(listt, ignore_index=True)
          except:
              print(sys.exc_info())
    except:
      print(sys.exc_info())
amz

amz.to_csv (r'amz_scraped_final.csv', index = False, header=True)

"""## **Scraping present prices using old datasets**"""

amz1=pd.read_csv('amazon_2020_03_09_1.csv')
mrp_now=[]
price_now=[]
for i in amz1['asin']:
  try:
    r = requests.get('https://www.amazon.in/dp/' + str(i))
    print(r.url)
    quote_page = str(r.url)
    page = urlopen(quote_page)
    time.sleep(2)
    soup = BeautifulSoup(page, 'html.parser')
    try:
      bbb = soup.find('span', attrs={'class': 'priceBlockStrikePriceString a-text-strike'}) # priceBlockStrikePriceString a-text-strike
      mrp_now.append(bbb.string)
    except:
      mrp_now.append('nill')
    try:
      ccc = soup.find('span', attrs={'class': 'a-size-medium a-color-price priceBlockBuyingPriceString'}) # a-size-medium a-color-price priceBlockBuyingPriceString
      price_now.append(ccc.string)
    except:
      price_now.append('nill')
  except:
    print(sys.exc_info())
    mrp_now.append('nill')
    price_now.append('nill')
amz1['mrp_now']=mrp_now
amz1['price_now']=price_now

print(len(amz1['mrp_now']))
amz11=amz1[amz1['mrp_now'] != amz1['mrp_now'][0]]
amz12=amz1[amz1['price_now'] != amz1['mrp_now'][0]]

amz2=pd.read_csv('amazon_final.csv')
mrp_now=[]
price_now=[]
for i in amz2['asin']:
  try:
    r = requests.get('https://www.amazon.in/dp/' + str(i))
    print(r.url)
    quote_page = str(r.url)
    page = urlopen(quote_page)
    time.sleep(2)
    soup = BeautifulSoup(page, 'html.parser')
    try:
      bbb = soup.find('span', attrs={'class': 'priceBlockStrikePriceString a-text-strike'}) # priceBlockStrikePriceString a-text-strike
      mrp_now.append(bbb.string)
    except:
      mrp_now.append('nill')
    try:
      ccc = soup.find('span', attrs={'class': 'a-size-medium a-color-price priceBlockBuyingPriceString'}) # a-size-medium a-color-price priceBlockBuyingPriceString
      price_now.append(ccc.string)
    except:
      price_now.append('nill')
  except:
    print(sys.exc_info())
    mrp_now.append('nill')
    price_now.append('nill')


amz2['mrp_now']=mrp_now
amz2['price_now']=price_now

print(len(amz2['mrp_now']))
amz21=amz2[amz2['mrp_now'] != amz2['mrp_now'][0]]
amz22=amz2[amz2['price_now'] != amz2['mrp_now'][0]]

frames=[amz11,amz12,amz21,amz22]
result=pd.concat(frames)
result=result.drop_duplicates(subset='asin')
result=result.reset_index(drop=True)
result.to_csv('amazon_pred_final.csv')

amz30k=pd.read_csv('marketing_sample_for_amazon_in_ecommerce_20191001_20191031_30k_data.csv')
amznew=pd.DataFrame(columns=list(amz30k.columns))
amz30k.reset_index()
words=['hand','soap','sanitizer','wipes', 'wash','handwash','bacteria', 'alcohol', 'hygiene']
for i in words:
    a=[amznew,amz30k[amz30k['product_title'].str.contains(i, regex=False)]]
    amznew=pd.concat(a)
amznew.drop_duplicates(subset ="product_asin", 
                     keep = False, inplace = True)
print(amznew.shape)
amznew.to_csv('amz30k_rel.csv')

amz3=pd.read_csv('amz30k_rel.csv')
mrp_now=[]
price_now=[]
for i in amz3['product_asin']:
  try:
    r = requests.get('https://www.amazon.in/dp/' + str(i))
    print(r.url)
    quote_page = str(r.url)
    page = urlopen(quote_page)
    soup = BeautifulSoup(page, 'html.parser')
    try:
      bbb = soup.find('span', attrs={'class': 'priceBlockStrikePriceString a-text-strike'}) # priceBlockStrikePriceString a-text-strike
      mrp_now.append(bbb.string)
    except:
      mrp_now.append('nill')
    try:
      ccc = soup.find('span', attrs={'class': 'a-size-medium a-color-price priceBlockBuyingPriceString'}) # a-size-medium a-color-price priceBlockBuyingPriceString
      price_now.append(ccc.string)
    except:
      price_now.append('nill')
  except:
    print(sys.exc_info())
    mrp_now.append('nill')
    price_now.append('nill')


amz3['mrp_now']=mrp_now
amz3['price_now']=price_now

amz31=amz3[amz3['mrp_now'] != amz3['mrp_now'][1]]
amz32=amz3[amz3['price_now'] != amz3['mrp_now'][1]]
frames=[amz31,amz32]
result=pd.concat(frames)
result=result.drop_duplicates(subset='product_asin')
result=result.reset_index(drop=True)
result.to_csv('amazon_30k_final.csv')

"""## **Flipkart Scarping : Scraping New info from Flipkart**"""

from urllib.request import urlopen
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
TAG_RE = re.compile(r'<[^>]+>')
kwords=['facemasks','ppe%20kit', 'handsanitizers','handwash']
amz = pd.DataFrame(columns=['url','product', 'name', 'mrp', 'discounted'])
pages=[1,2,3,4,5,6,7,8,9]
def remove_tags(text):
    return TAG_RE.sub('', text)
url_p1 = 'https://www.flipkart.com/search?q='
url_p2='&page='
for j in kwords:
  for k in pages: 
        page = urlopen(str(url_p1)+j+str(url_p2)+str(k))
        soup = BeautifulSoup(page, 'html.parser')
        raw_urls = soup.find_all('a', attrs={'class': '_2cLu-l'})
        raw_urls=list(raw_urls)
        for i in raw_urls:
                try:
                        r = requests.get('https://www.flipkart.com' + str(i['href']))
                        #print(r.url)
                        quote_page = str(r.url)
                        page = urlopen(quote_page)
                        soup = BeautifulSoup(page, 'html.parser')
                        try:
                          aaa = soup.find_all('span', attrs={'class': '_35KyD6'})
                          aaa=remove_tags(str(aaa[0]))
                        except:
                          aaa='nill'
                        try:
                          bbb = soup.find_all('div', attrs={'class': '_3auQ3N _1POkHg'})
                          bbb=remove_tags(str(bbb[0]))
                        except:
                          bbb='nill'
                        try:
                          ccc = soup.find('div', attrs={'class': '_1vC4OE _3qQ9m1'})
                          ccc =ccc.string
                        except:
                          ccc='nill'
                        print(aaa)
                        print(bbb)
                        print(ccc)
                        listt=[r.url, j,aaa,bbb,ccc]
                        listt = pd.Series(listt, index = amz.columns)
                        amz = amz.append(listt, ignore_index=True)
                except:
                        pass
amz

amz.to_csv('flipkart_pandemic_products.csv', index=False)

"""## **Scraping Amazon Stars**"""

amzz=pd.read_csv('amazon_scraped_final.csv')
starlist=[]
count=0
for k in amzz['url'][67:]:
  x=0
  print(str(count)+" ::")
  while (x<=3):
    try:
      time.sleep(2)
      # r = requests.get(k)
      # print(r.url)
      # quote_page = str(r.url)
      page = urlopen(k)
      soup = BeautifulSoup(page, 'html.parser')
      print(k)
      try:
        star1 = soup.find('a', attrs={'class': 'a-link-normal 1star'})
        star1=star1['title']
      except:
        star1='nill'
      try:
        star2 = soup.find('a', attrs={'class': 'a-link-normal 2star'})
        star2=star2['title']
      except:
        star2='nill'
      try:
        star3 = soup.find('a', attrs={'class': 'a-link-normal 3star'})
        star3=star3['title']
      except:
        star3='nill'
      try:
        star4 = soup.find('a', attrs={'class': 'a-link-normal 4star'})
        star4=star4['title']
      except:
        star4='nill'
      try:
        star5 = soup.find('a', attrs={'class': 'a-link-normal 5star'})
        star5=star5['title']
      except:
        star5='nill'
      listt=[star1, star2, star3, star4, star5]
      starlist.append(listt)
      x=4
    except:
      print('try :'+str(x)+' '+str(sys.exc_info()) +' : ' + str(k))
      x=x+1
      if (x == 4):
        starlist.append(['nill', 'nill', 'nill', 'nill', 'nill'])
        print('null appended')
  count=count+1

"""### **===================================**"""

amzz=pd.read_csv('amz_pred_final.csv')
starlist=[]
count=0
for k in amzz['product_url']:
  x=0
  print(str(count)+" ::")
  while (x<=3):
    try:
      time.sleep(2)
      # r = requests.get('https://www.amazon.in/dp/'+k)
      # print(r.url)
      # urll = str(r.url)
      urll=k
      page = urlopen(urll)
      soup = BeautifulSoup(page, 'html.parser')
      print(urll)
      try:
        star1 = soup.find('a', attrs={'class': 'a-link-normal 1star'})
        star1=star1['title']
      except:
        star1='nill'
      try:
        star2 = soup.find('a', attrs={'class': 'a-link-normal 2star'})
        star2=star2['title']
      except:
        star2='nill'
      try:
        star3 = soup.find('a', attrs={'class': 'a-link-normal 3star'})
        star3=star3['title']
      except:
        star3='nill'
      try:
        star4 = soup.find('a', attrs={'class': 'a-link-normal 4star'})
        star4=star4['title']
      except:
        star4='nill'
      try:
        star5 = soup.find('a', attrs={'class': 'a-link-normal 5star'})
        star5=star5['title']
      except:
        star5='nill'
      listt=[star1, star2, star3, star4, star5]
      starlist.append(listt)
      x=4
    except:
      print('try :'+str(x)+' '+str(sys.exc_info()) +' : ' + str(urll))
      x=x+1
      if (x == 4):
        starlist.append(['nill', 'nill', 'nill', 'nill', 'nill'])
        print('null appended')
  count=count+1

starlist.count(['nill', 'nill', 'nill', 'nill', 'nill'])

import re
amzz['star1']=[list(map(int, re.findall(r'\d+', i[0])))[0] if (i[0]!='nill') else 0 for i in starlist]
amzz['star2']=[list(map(int, re.findall(r'\d+', i[1])))[0] if (i[1]!='nill') else 0 for i in starlist]
amzz['star3']=[list(map(int, re.findall(r'\d+', i[2])))[0] if (i[2]!='nill') else 0 for i in starlist]
amzz['star4']=[list(map(int, re.findall(r'\d+', i[3])))[0] if (i[3]!='nill') else 0 for i in starlist]
amzz['star5']=[list(map(int, re.findall(r'\d+', i[4])))[0] if (i[4]!='nill') else 0 for i in starlist]

amzz

amzz.to_csv('amazon_pred_final_stars.csv', index=False)

"""## **Scraping Flipkart Stars**"""

import re
import xml.etree.ElementTree


TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return list(xml.etree.ElementTree.fromstring(text).itertext())

amzz=pd.read_csv('flipkart_pandemic_products.csv')
starlist=[]
count=0
for k in amzz['url']:
  try:
    print(str(count)+" ::")
    #time.sleep(2)
    urll=k
    page = urlopen(urll)
    soup = BeautifulSoup(page, 'html.parser')
    star1 = soup.find('ul', attrs={'class': '_2M5FGu'})
    print(    remove_tags(str(star1)))
    starlist.append(remove_tags(str(star1)))
    print("success : "+str(urll))
  except:
    print("failiure : "+str(urll))
    starlist.append(['nil', 'nil', 'nil', 'nil', 'nil'])
  count=count+1

starlist.count(['nil', 'nil', 'nil', 'nil', 'nil'])

amzz['star5']=[i[0].replace(',', '') if (i[0]!='nill') else 'nill' for i in starlist]
amzz['star4']=[i[1].replace(',', '') if (i[0]!='nill') else 'nill' for i in starlist]
amzz['star3']=[i[2].replace(',', '') if (i[0]!='nill') else 'nill' for i in starlist]
amzz['star2']=[i[3].replace(',', '') if (i[0]!='nill') else 'nill' for i in starlist]
amzz['star1']=[i[4].replace(',', '') if (i[0]!='nill') else 'nill' for i in starlist]

amzz.to_csv('flipkart_pandemic_products_with_stars.csv', index=False)